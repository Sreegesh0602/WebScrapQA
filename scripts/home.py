import os
import sys
import streamlit as st
import numpy as np
import pandas as pd

# Ensure the parent directory is in sys.path to resolve modules like config.settings
parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from config.settings import RAW_DATA_DIR, OUTPUT_DIR, WIKI_TOPIC, WIKI_OUTPUT_FILE
from src.text_processor import get_wikipedia_content, write_text_to_file
from src.data_loader import load_documents, split_documents, documents_to_dataframe
from src.graph_handler import df_to_graph, initialise_neo4j_schema, insert_dataframe_to_neo4j
from src.create_graph import generate_chain

# ğŸŒŸ --- Streamlit UI ---
st.set_page_config(page_title="Graph Query Pipeline", page_icon="ğŸ•µï¸â€â™‚ï¸", layout="wide")

# ğŸŒŸ Sidebar
with st.sidebar:
    # st.image("https://upload.wikimedia.org/wikipedia/commons/8/8e/Neo4j-logo.png", width=200)
    st.markdown("### âš¡ **Graph Query Pipeline**")
    st.info("This app extracts Wikipedia content, builds a Neo4j graph, and translates natural language into Cypher queries.")
    # st.markdown("#### ğŸ”— **Quick Links**")
    # st.markdown("[ğŸ“– Neo4j Docs](https://neo4j.com/docs/)")
    # st.markdown("[ğŸ›  LangChain](https://python.langchain.com/)")
    # st.markdown("[ğŸ“Š Streamlit](https://streamlit.io/)")

# ğŸŒŸ Header Section
st.title("ğŸ•µï¸ Graph Query Pipeline")
st.subheader("ğŸ” Extract insights from Wikipedia, build a knowledge graph, and explore relationships!")

# âœ… Step 1: Get Wikipedia content
with st.expander("ğŸ“– Fetch Wikipedia Content"):
    if not os.path.exists(WIKI_OUTPUT_FILE):
        st.info("Fetching content from Wikipedia... â³")
        wiki_text = get_wikipedia_content(WIKI_TOPIC)
        write_text_to_file(WIKI_OUTPUT_FILE, wiki_text)
        st.success("âœ… Wikipedia content saved successfully.")
    else:
        st.success("âœ… Wikipedia content already exists.")

# âœ… Step 2: Load and process documents
with st.expander("ğŸ“‚ Load & Process Documents"):
    documents = load_documents(RAW_DATA_DIR)
    pages = split_documents(documents)
    df_chunks = documents_to_dataframe(pages)

    st.success("âœ… Documents successfully loaded and split into pages!")
    st.write("ğŸ“Š **Data Preview:**")
    st.dataframe(df_chunks.head(5))

# âœ… Step 3: Create or load graph DataFrame
with st.expander("ğŸ“Š Generate Graph DataFrame"):
    graph_csv = os.path.join(OUTPUT_DIR, "graph.csv")

    if not os.path.exists(graph_csv):
        st.info("Generating graph DataFrame... â³")
        df_graph = df_to_graph(df_chunks.head(1), model="llama3")
        df_graph.replace("", np.nan, inplace=True)
        df_graph["count"] = 4  # Default count value

        os.makedirs(OUTPUT_DIR, exist_ok=True)
        df_graph.to_csv(graph_csv, index=False)
        df_chunks.to_csv(os.path.join(OUTPUT_DIR, "chunks.csv"), index=False)

        st.success("âœ… Graph data saved!")
    else:
        st.success("âœ… Graph CSV already exists, loading data...")
        df_graph = pd.read_csv(graph_csv)

    st.write("ğŸ“Œ **Graph Data Preview:**")
    st.dataframe(df_graph.head(5))

# âœ… Step 4: Initialize Neo4j Schema & Insert Data
with st.expander("ğŸ›  Initialize & Insert Data into Neo4j"):
    st.info("Setting up Neo4j schema & inserting data...")
    initialise_neo4j_schema()
    insert_dataframe_to_neo4j(df_graph)
    st.success("âœ… Data successfully inserted into Neo4j!")

# âœ… Step 5: Query the Graph using Natural Language
st.markdown("---")
st.subheader("ğŸ” Query the Knowledge Graph")

query = st.text_input("âœï¸ **Enter your graph query:**", "")

if st.button("ğŸš€ Run Query"):
    with st.spinner("Processing your query... â³"):
        chain = generate_chain()
        response = chain.invoke(query)

        # Extract full context from the response
        full_context = response.get("full_context") or response.get("result", [])

        # âœ… Display Results in a Better UI
        if isinstance(full_context, list):
            st.subheader("ğŸ“Š **Extracted Relationships**")
            for item in full_context:
                n = item.get("n", {})
                relatedNode = item.get("relatedNode", {})
                relationshipType = item.get("type(r)", "Unknown")
                relationshipProperties = item.get("properties(r)", {})

                with st.expander(f"ğŸ”— {n.get('name', 'Unknown')} â {relatedNode.get('name', 'Unknown')}"):
                    st.markdown(f"**ğŸ”¹ Entity:** `{n.get('name', 'Unknown')}` ({n.get('category', 'N/A')})")
                    st.markdown(f"**ğŸ”¹ Relationship:** `{relationshipType}`")
                    st.markdown(f"**ğŸ”¹ Related To:** `{relatedNode.get('name', 'Unknown')}` ({relatedNode.get('category', 'N/A')})")
                    st.markdown(f"**ğŸ”¹ Relationship Details:** `{relationshipProperties.get('relationship', 'No details')}`")

        elif isinstance(full_context, str):
            st.subheader("ğŸ“œ **Summary Response**")
            st.write(full_context)
        else:
            st.error("âš ï¸ Unexpected response format!")
            st.write(response)

st.markdown("---")
st.markdown("ğŸ‘¨â€ğŸ’» **Built with â¤ï¸ using Streamlit, Neo4j & LangChain** ğŸš€")
